{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batch Stochastic Gradient Descent\n",
    "\n",
    "1. Draw a bacth of training samples x and corresponding target y. \n",
    "2. Run the network on x to obtain predictions y_pred.\n",
    "3. Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y.\n",
    "4. Compute the backward pass i.e. gradients of the loss with respect to model parameters. \n",
    "5. Move the parameters a little in the opposite direction from the gradient - \n",
    "\n",
    "> W -= step * gradient\n",
    "\n",
    "thus reducing the loss on the batch a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch SGD\n",
    "\n",
    "Instead of taking a sample of data, and performing the SGD on entire data at once, is called Batch SGD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "Taking a single datapoint and running the SGD would mean simple SGD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Optimizers\n",
    "\n",
    "There exists variations of SGD that differ by taking into account previous weight updates when computing the nexr weight update, rather than just looking at the current value of the gradients. \n",
    "\n",
    "    There is for instance SGD with momentum, Adagrad, RMSProp, and several others. \n",
    "    \n",
    "**Momentum** addresses two major issues with SGD: convergence speed and local minima. Naive way to understand momentum would be that the weight update not only involves current gradient, but also includes part of previous parameter updates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "\n",
    "Backpropogation is nothing but the chain rule of derivates applied repeatedly to obtain the required weight updates or gradients. \n",
    "The Backpropogation starts at the calculation of loss, and then is propogated back till every parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

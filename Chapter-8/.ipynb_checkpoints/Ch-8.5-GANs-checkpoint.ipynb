{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### Generative Adverserial Networks\n",
    "\n",
    "-----\n",
    "\n",
    "- **Generator Network** - Takes as input a random vector(random point in latent space), and decodes it tnto synthetic image. \n",
    "- **Discriminator Network(or adversary)** - Takes an input as an image(real or synthetic), and predicts whether the image came from the training set or was created by Discriminator Network. \n",
    "- The generator network is trained to be able to fool the discriminator network, and thus it evolves towards generating increasingly realistic images as training goes on:artificial images that look indistinguishable from real ones, to the extent the discriminator is able to tell apart. \n",
    "- The discriminator is also trained to adapt to gradually improving capabilities of the generator network, setting a higher bar of realism for the generated images. \n",
    "- Once the training is over the Generator is capable of converting any point in latent space into believable image. Unlike VAEEs, this latent space has fewer explicit guarantees of meaningful structure, it isn't continous. \n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "- In GAN we the optimization minimum is not fixed. \n",
    "- Gradient descent is rolling down hills in a static loss landscape. \n",
    "- But with GANs, every step taken down the hill changes the entire landscape a little.\n",
    "- **It is a dynamic system where the optimization process is seeking not a minimum, but a equilibrium between two forces. FOr this reason, GANs are notoriously difficult to train-getting a GAN to work requires lots of careful tuning of the model architecture and training parameters.**\n",
    "\n",
    "------\n",
    "\n",
    "***Schematic GAN implementation***\n",
    "\n",
    "-----\n",
    "\n",
    "Here we use Conv2DTranspose to upsample the images. Also in the following section we will be discussing how to define GAN for CIFAR dataset. \n",
    "\n",
    "1. A generator network maps vectors of shape (latent_dim,) to images of shape (32,32,3). \n",
    "2. A discriminator network maps images of shape (32,32,3) to a binary score estimating the probability that the image is real. \n",
    "3. A GAN network chains the above networks together: \n",
    "```python\n",
    "gan(x) = discriminator(generator(x))\n",
    "```\n",
    "Thus this gan network maps latenet space vector's to discriminators assessment of the realism of these latent vectors as decoded by generator. \n",
    "4. You train the discriminator using examples of real and fake images along with \"real/fake\" labels, just as you train any regular image-classification model.\n",
    "5. To train the generator, you use the gradients of the generator's weights with regard to the loss of the gan model. This means that at every step, you move the weights of the generator in a direction that makes the discriminator more likely to classify as \"real\" the images created by the generator. Esentially we train the generator to fool the discriminator. \n",
    "\n",
    "\n",
    "--------\n",
    "\n",
    "***Some tips before jumping into implementation***\n",
    "\n",
    "------\n",
    "- We use tanh as the last activation in generator, instead of sigmoid, which is more commonly found in other types of models. \n",
    "- We sample points from latent space using a Normal distribution, not a uniform distribution. \n",
    "- Stochasticity is good to induce robustness. Becasue the GAN results in dynamic equilibrium, GANs are likely to get stuck in a lot of ways. **Introducing randomness during training helps prevent this. We introduce randomness in two ways: by using dropout in the discriminator and by adding random noise to the labels for the discriminator.** \n",
    "- Sparse gradients can hinder GAN training. In deep learning, sparsity is often a desirable property, but not in GANs. Two things that induce gradient sparsity: max-poolong and ReLU activation. **Instead of max pooling, we recommend using a strided convolutionsfor downsampling, and we recommend using a LeakyReLU layer instead of ReLU. It is similar to ReLU but relaxes sparsity by allowing small negative activation values.**\n",
    "- IN generated images, it;s common to see checkerboard artifacts caused by unequal coverage of the pixel space in the generator. To fix this, **we use a kernal size that's divisible by the stride size**, whenever we use a strided Conv2DTranspose or Conv2D in both generator and the discriminator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "***Let's move to the implementation***\n",
    "\n",
    "---------\n",
    "\n",
    "***The generator Network***\n",
    "\n",
    "--------\n",
    "\n",
    "A lot of time generator gets stuck with generated images that look like noise. **So, it's better to use dropout in both generator and discriminator networks**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = layers.Input(shape = (latent_dim,))\n",
    "\n",
    "### TRANSFORMS THE INPUT TO A 16X16, 128 CHANNEL FEATURE MAP\n",
    "x = layers.Dense(128*16*16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16,16,128))(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding = 'same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "### UPSAMPLES TO 32*32\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding = 'same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding = 'same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(channels, 7, activation = 'tanh', padding = 'same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = keras.models.Model(generator_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579.0\n",
      "Trainable params: 6,264,579.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "***Discriminator Network***\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yash/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "discriminator_input = layers.Input(shape=(height, width,channels))\n",
    "\n",
    "x = layers.Conv2D(128,3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(1, activation = 'sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = keras.models.Model(discriminator_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913.0\n",
      "Trainable params: 790,913.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To stabalize training, use learning rate decay \n",
    "### Use gradient clipping\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(\n",
    "    lr = 0.0008,\n",
    "    clipvalue = 1.0,\n",
    "    decay=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yash/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "discriminator.compile(optimizer = discriminator_optimizer, \n",
    "                     loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "***The adversarial Network***\n",
    "\n",
    "-------\n",
    "\n",
    "**Now we will setup GAN. The model turns latent space points into a clssification decision - 'fake' or 'real'- and it is meant to be trained with labels \"these are real images\".**\n",
    "\n",
    " - It is important to freeze the discriminator during training. It's weights won't be updated while training GAN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = layers.Input(shape = (latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = keras.models.Model(gan_input, gan_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 32, 32, 3)         6264579   \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 1)                 790913    \n",
      "=================================================================\n",
      "Total params: 7,055,492.0\n",
      "Trainable params: 7,055,492.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_optimizer = keras.optimizers.RMSprop(lr = 0.0004, clipvalue = 1.0, decay = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(gan_optimizer, loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "***Training the DCGAN***\n",
    "\n",
    "----\n",
    "\n",
    "For each epoch we will do the following - \n",
    "1. Draw random points form latent space. \n",
    "2. Generate images using generator network by feeding the random points.\n",
    "3. Mix generated images and real images.\n",
    "4. Train Discriminator using these mixed images, with corresponding targets: either 'real' for real images and 'fake' for fake images. \n",
    "5. Draw new points from latent space. \n",
    "6. TRain gan using the newly extracted points from latent space. Here we will pass the targets as 'real'. This will train the generator(here the discriminator will be frozen). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (_,_) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selecting only frog images\n",
    "x_train = x_train[y_train.flatten() == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Normalization\n",
    "x_train = x_train.reshape((x_train.shape[0],) + (height, width,channels)).astype('float32')/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "batch_size = 20\n",
    "save_dir = 'gan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "discriminator loss: 0.67374575\n",
      "adverserial loss: 0.6432811\n",
      "discriminator loss: 0.68710434\n",
      "adverserial loss: 0.8233681\n",
      "discriminator loss: 0.6849948\n",
      "adverserial loss: 0.78601927\n",
      "discriminator loss: 0.6934394\n",
      "adverserial loss: 0.7108942\n",
      "discriminator loss: 0.6998938\n",
      "adverserial loss: 0.7576715\n",
      "discriminator loss: 0.69417727\n",
      "adverserial loss: 0.7376522\n",
      "discriminator loss: 0.6934203\n",
      "adverserial loss: 0.75727564\n",
      "discriminator loss: 0.67730457\n",
      "adverserial loss: 0.9952731\n",
      "discriminator loss: 0.68475705\n",
      "adverserial loss: 0.7456008\n",
      "discriminator loss: 0.6937794\n",
      "adverserial loss: 0.78125745\n",
      "discriminator loss: 0.7264334\n",
      "adverserial loss: 0.74640864\n",
      "discriminator loss: 0.7057308\n",
      "adverserial loss: 0.7664032\n",
      "discriminator loss: 0.69312286\n",
      "adverserial loss: 0.7578683\n",
      "discriminator loss: 0.7026157\n",
      "adverserial loss: 0.73570263\n",
      "discriminator loss: 0.6921697\n",
      "adverserial loss: 0.7253946\n",
      "discriminator loss: 0.70758694\n",
      "adverserial loss: 0.76370525\n",
      "discriminator loss: 0.6841394\n",
      "adverserial loss: 0.7474088\n",
      "discriminator loss: 0.6748674\n",
      "adverserial loss: 0.782827\n",
      "discriminator loss: 0.69515103\n",
      "adverserial loss: 0.74324316\n",
      "discriminator loss: 0.7951823\n",
      "adverserial loss: 0.91691625\n",
      "discriminator loss: 0.6931181\n",
      "adverserial loss: 0.7469841\n",
      "discriminator loss: 0.69466704\n",
      "adverserial loss: 0.7499903\n",
      "discriminator loss: 0.69745713\n",
      "adverserial loss: 0.7226858\n",
      "discriminator loss: 0.6817792\n",
      "adverserial loss: 0.7317342\n",
      "discriminator loss: 0.6858168\n",
      "adverserial loss: 0.81000364\n",
      "discriminator loss: 0.69768256\n",
      "adverserial loss: 0.72052735\n",
      "discriminator loss: 0.70254266\n",
      "adverserial loss: 0.8998249\n",
      "discriminator loss: 0.6870845\n",
      "adverserial loss: 0.76006395\n",
      "discriminator loss: 0.68375385\n",
      "adverserial loss: 0.7720934\n",
      "discriminator loss: 0.70474625\n",
      "adverserial loss: 0.7564392\n",
      "discriminator loss: 0.6909472\n",
      "adverserial loss: 0.752106\n",
      "discriminator loss: 0.71234465\n",
      "adverserial loss: 0.82156575\n",
      "discriminator loss: 0.69743645\n",
      "adverserial loss: 0.75856006\n",
      "discriminator loss: 0.6907604\n",
      "adverserial loss: 0.75262415\n",
      "discriminator loss: 0.6832169\n",
      "adverserial loss: 0.7589566\n",
      "discriminator loss: 0.6906382\n",
      "adverserial loss: 0.7070931\n",
      "discriminator loss: 0.6843251\n",
      "adverserial loss: 0.745246\n",
      "discriminator loss: 0.7491573\n",
      "adverserial loss: 0.750352\n",
      "discriminator loss: 0.68635863\n",
      "adverserial loss: 0.7403012\n",
      "discriminator loss: 0.6872405\n",
      "adverserial loss: 0.73252547\n",
      "discriminator loss: 0.6964801\n",
      "adverserial loss: 0.7609134\n",
      "discriminator loss: 0.6983398\n",
      "adverserial loss: 0.6035699\n",
      "discriminator loss: 0.6937706\n",
      "adverserial loss: 0.7364898\n",
      "discriminator loss: 0.6739079\n",
      "adverserial loss: 0.6855922\n",
      "discriminator loss: 0.71920836\n",
      "adverserial loss: 0.65509886\n",
      "discriminator loss: 0.70535755\n",
      "adverserial loss: 0.7330365\n",
      "discriminator loss: 0.6985358\n",
      "adverserial loss: 0.7830933\n",
      "discriminator loss: 0.6965796\n",
      "adverserial loss: 0.7523883\n",
      "discriminator loss: 0.7063142\n",
      "adverserial loss: 0.7859772\n",
      "discriminator loss: 0.7161215\n",
      "adverserial loss: 0.7056579\n",
      "discriminator loss: 0.6769694\n",
      "adverserial loss: 0.84484607\n",
      "discriminator loss: 0.64895964\n",
      "adverserial loss: 0.7598356\n",
      "discriminator loss: 0.7030941\n",
      "adverserial loss: 0.80336845\n",
      "discriminator loss: 0.69761246\n",
      "adverserial loss: 0.70270646\n",
      "discriminator loss: 0.7009213\n",
      "adverserial loss: 0.96688426\n",
      "discriminator loss: 0.69066894\n",
      "adverserial loss: 0.8172892\n",
      "discriminator loss: 0.6812619\n",
      "adverserial loss: 0.75054055\n",
      "discriminator loss: 0.7200218\n",
      "adverserial loss: 0.8402785\n",
      "discriminator loss: 0.70037836\n",
      "adverserial loss: 0.7603635\n",
      "discriminator loss: 0.69616157\n",
      "adverserial loss: 0.844543\n",
      "discriminator loss: 0.7213348\n",
      "adverserial loss: 0.8052368\n",
      "discriminator loss: 0.6895647\n",
      "adverserial loss: 0.8374537\n",
      "discriminator loss: 0.75592476\n",
      "adverserial loss: 1.0426595\n",
      "discriminator loss: 0.6577401\n",
      "adverserial loss: 0.88800776\n",
      "discriminator loss: 0.67227733\n",
      "adverserial loss: 0.6543702\n",
      "discriminator loss: 0.6968652\n",
      "adverserial loss: 0.7365941\n",
      "discriminator loss: 0.656114\n",
      "adverserial loss: 1.0022495\n",
      "discriminator loss: 0.7148445\n",
      "adverserial loss: 0.7351988\n",
      "discriminator loss: 0.6513719\n",
      "adverserial loss: 0.7605506\n",
      "discriminator loss: 0.67222273\n",
      "adverserial loss: 0.8294608\n",
      "discriminator loss: 0.69077355\n",
      "adverserial loss: 0.7330842\n",
      "discriminator loss: 0.7083926\n",
      "adverserial loss: 0.8579931\n",
      "discriminator loss: 0.7169925\n",
      "adverserial loss: 0.9835167\n",
      "discriminator loss: 0.6886319\n",
      "adverserial loss: 0.7932906\n",
      "discriminator loss: 0.7123083\n",
      "adverserial loss: 0.73658717\n",
      "discriminator loss: 0.6823752\n",
      "adverserial loss: 0.86003244\n",
      "discriminator loss: 0.67292464\n",
      "adverserial loss: 0.8386427\n",
      "discriminator loss: 0.6922326\n",
      "adverserial loss: 0.89027596\n",
      "discriminator loss: 0.6628723\n",
      "adverserial loss: 0.73864406\n",
      "discriminator loss: 0.7097141\n",
      "adverserial loss: 0.68702376\n",
      "discriminator loss: 0.72116786\n",
      "adverserial loss: 0.8698305\n",
      "discriminator loss: 0.64699423\n",
      "adverserial loss: 0.7412855\n",
      "discriminator loss: 0.6629225\n",
      "adverserial loss: 0.87074316\n",
      "discriminator loss: 0.67904794\n",
      "adverserial loss: 0.82041466\n",
      "discriminator loss: 0.749164\n",
      "adverserial loss: 1.532718\n",
      "discriminator loss: 0.6543306\n",
      "adverserial loss: 0.84584314\n",
      "discriminator loss: 0.67941535\n",
      "adverserial loss: 0.6178096\n",
      "discriminator loss: 0.7079302\n",
      "adverserial loss: 0.8074616\n",
      "discriminator loss: 0.6830276\n",
      "adverserial loss: 0.83844376\n",
      "discriminator loss: 0.7066122\n",
      "adverserial loss: 0.69234115\n",
      "discriminator loss: 0.6841318\n",
      "adverserial loss: 0.8612816\n",
      "discriminator loss: 0.6794243\n",
      "adverserial loss: 0.826402\n",
      "discriminator loss: 0.7400242\n",
      "adverserial loss: 0.98063153\n",
      "discriminator loss: 0.6652592\n",
      "adverserial loss: 0.81095904\n",
      "discriminator loss: 0.76207143\n",
      "adverserial loss: 0.9633514\n",
      "discriminator loss: 0.6942519\n",
      "adverserial loss: 0.7328967\n",
      "discriminator loss: 0.6751626\n",
      "adverserial loss: 0.6744178\n",
      "discriminator loss: 0.68821734\n",
      "adverserial loss: 0.8442062\n",
      "discriminator loss: 0.67427826\n",
      "adverserial loss: 0.77068365\n",
      "discriminator loss: 0.6986526\n",
      "adverserial loss: 0.7825948\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "for step in range(iterations):\n",
    "    ### Generating random latent samples from Normal Distribution \n",
    "    random_latent_vectors = np.random.normal(size = (batch_size, latent_dim))\n",
    "    \n",
    "    ### Generator generates fake images\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    \n",
    "    ### Sampling real images from training data\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start:stop]\n",
    "    \n",
    "    ### Creating dataset to train Discriminator\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "    ### Add noise to labels(Important)\n",
    "    labels += 0.05*np.random.random(labels.shape)\n",
    "    \n",
    "    ### Train the discriminator\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "    \n",
    "    ###############################################################################\n",
    "    \n",
    "    random_latent_vectors = np.random.normal(size = (batch_size, latent_dim))\n",
    "    misleading_targets = np.zeros((batch_size,1))\n",
    "    ### Train generator (Here the discriminator is frozen)\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        gan.save_weights('gan/gan.h5')\n",
    "        print('discriminator loss:', d_loss)\n",
    "        print('adverserial loss:', a_loss)\n",
    "        \n",
    "        img = image.array_to_img(generated_images[0]*255. , scale = False)\n",
    "        img.save(os.path.join(save_dir, 'gen' + str(step) + '.png'))\n",
    "        \n",
    "        img = image.array_to_img(real_images[0]*255. , scale = False)\n",
    "        img.save(os.path.join(save_dir, 'real' + str(step) + '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, we may see adverserial loss begin to increase considerably, while discriminator loss tends to zero-the discriminator may end up dominating the generator. In this case we can reduce the learning rate of the discriminator and increase it's dropout value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

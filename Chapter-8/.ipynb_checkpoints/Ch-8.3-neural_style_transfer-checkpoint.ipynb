{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Neural Style Transfer \n",
    "\n",
    "-----\n",
    "\n",
    "- Neural Style transfer consists of applying the style of a reference image to a target image while conserving the content of the target image.\n",
    "- Style means, textures, colors and visual patterns in the image at various scales; and the content is the higher level macro-structure of the image.\n",
    "- Example, in the starry night by Van Gogh the style refers to the blue and yellow circular brushstrokes. \n",
    "- The root of the algorithm is the same as all the deep learning algorithms, i.e. defining a meaningful loss function and then optimizing it. \n",
    "- If were able to mathematically define \"content\" and \"style\" then the loss function would look like following\n",
    "\n",
    "> loss = distance(style(reference_image) - style(generated_image)) + distance(content(original_image) - content(generated_image))\n",
    "\n",
    "So, how do we define the content and style mathematically?\n",
    "\n",
    "\n",
    "***The content loss***\n",
    "\n",
    "- Activations from earlier layers in the network contain local information about the image, whereas activations from higher layers contain increasingly global abstract information. \n",
    "- In other words, the activations of different layers of a convnet provide a decomposition of the contents of an image over different spatial scales.\n",
    "- **So a good candidate for content loss is thus the L2 Norm between the activations of an upper layer in a pretrained convnet, computed over target image, and the activations of the same layer computed over generated image.**\n",
    "- The content loss only uses a single upper layer. \n",
    "\n",
    "***The style loss***\n",
    "\n",
    "- The style loss uses multiple layers of a convnet. \n",
    "- We try to capture the appearence of the style reference image at all spatial scales extracted by the convnet. \n",
    "- For style loss, **Gram Matrix** of a layer's activations: the inner product of the feature maps of given layer. \n",
    "- Gram Matrix equation is very similar to Co-relation equation, the major differnce being that the means are not subtracted while calculating Gram matrix. \n",
    "- Inner product can be understood as representing a map of the correlations between layer's features.\n",
    "- These feature correlations capture the statistics of the patterns of a particular spatial scale, which emperically correspond to textures found at this scale. \n",
    "- **The style loss aims to preserve similar internal correlations within the activations of different layers, across the style reference image and the generated image. In turn this guarantees that the textures found at different spatial scales look similar across the style-reference image and generated image.**\n",
    "\n",
    "-----\n",
    "\n",
    "***Keras Implementation of Style-Transfer***\n",
    "\n",
    "-----\n",
    "\n",
    "We will use a pre-trained network to get started with the pretrained convnet. As the original paper used VGG19, we will also use the same. \n",
    "\n",
    "- Set up the network that computes VGG19 layer activations for the style reference image, the target image, and the generated image at the same time. \n",
    "- Use the layer activations computed over three images to define the loss function, which we will minimize. \n",
    "- Setup gradient descent process to minimize the loss function. \n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "***Defining initial Variable***\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "target_image_path = 'style_transfer/image_scene.jpg'\n",
    "style_reference_image_path = 'style_transfer/starry_night.jpg'\n",
    "\n",
    "### Defining dimensions for the generated image\n",
    "width, height = load_img(target_image_path).size \n",
    "img_height = 400\n",
    "img_width = int(width*img_height/height) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "***Auxillary Functions***\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.applications import vgg19\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size = (img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    ### Zero cenetering by removing the mean pixel value from imagenet.\n",
    "    ### Reverses the transformation done by vgg19\n",
    "    x[:,:,0] += 103.939\n",
    "    x[:,:,1] += 116.779\n",
    "    x[:,:,2] += 123.68\n",
    "    ### Converting BGR back to RGB\n",
    "    ### Also a preprocess step in vgg19 which is being reversed\n",
    "    x = x[:,:,::-1]\n",
    "    x = np.clip(x,0,255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's setup the VGG19 network. \n",
    "\n",
    "- It takes as input set of three images, style reference image, target image, and placeholder for generated image. \n",
    "- The first two are static and thus will be defined using K.constant, whereas the third tensor's value will keep on changing so it is added in a placeholder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
    "combination_image = K.placeholder((1, img_height, img_width, 3))\n",
    "\n",
    "input_tensor = K.concatenate([target_image, style_reference_image,combination_image], axis=0)\n",
    "\n",
    "model = vgg19.VGG19(input_tensor = input_tensor, \n",
    "                   weights = 'imagenet', \n",
    "                   include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining content loss\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining style loss\n",
    "\n",
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x,(2,0,1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram \n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3 \n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S-C)) / (4. * (channels**2) * (size ** 2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "***Total Variation loss*** - This is the third type of loss which operates on pixels of generated combination image. It encourages spatial ocntinuity, thus avoiding overly pixelated results. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = K.square(\n",
    "        x[:,:img_height-1, :img_width-1,:]-\n",
    "        x[:,1:,:img_width-1,:]\n",
    "    )\n",
    "    b = K.square(\n",
    "        x[:,:img_height-1, :img_width-1,:]-\n",
    "        x[:,:img_height-1, 1:,:]\n",
    "    )\n",
    "    return K.sum(K.pow(a+b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "***The loss we minimize is the weighted average of these three losses***\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer = 'block5_conv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "total_variation_weight = 1e-4\n",
    "style_weight = 1.\n",
    "content_weight = 0.025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yash/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1044: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "loss = K.variable(0.)\n",
    "layer_features = outputs_dict[content_layer]\n",
    "target_image_features = layer_features[0,:,:,:]\n",
    "combination_features = layer_features[2,:,:,:]\n",
    "loss += content_weight * content_loss(target_image_features, combination_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yash/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "for layer_name in style_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1,:,:,:]\n",
    "    combination_features = layer_features[2,:,:,:]\n",
    "    s1 = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight/len(style_layers)) * s1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss += total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "***Now, we will start with Gradient Descent, using the L-BFGS Algorithm***\n",
    "\n",
    "-----\n",
    "\n",
    "- It is an inbuilt function in Scipy but it only works for 1D vectors, whereas we are working with 3D Image Tensor. \n",
    "- Or it requires you to pass the loss and gradients are two seperate functions. \n",
    "\n",
    "So, we will write a class which calculates both the loss and gradient which can be passed to the Scipy function later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gets the gradients of generated image with respect to loss\n",
    "grads = K.gradients(loss, combination_image)[0]\n",
    "\n",
    "### Function to fetch the current values of loss and gradients\n",
    "fetch_loss_and_grads = K.function([combination_image], [loss,grads])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basically we are have a single function which returns loss and grads, \n",
    "### Instead of calling the function fetch_loss_and_grad twice the following class will\n",
    "### help us cache the value and divide the loss and gradient function\n",
    "\n",
    "class Evaluator(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "    \n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, img_height, img_width, 3))\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "    \n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "    \n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "***Now we will run the Gradient ascent using the Scipy's L-BFGS Algorithm***\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_prefix = 'style_transfer/my_result'\n",
    "iterations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### As scipy L-BFGS can process only flat vectors\n",
    "\n",
    "x = preprocess_image(target_image_path)\n",
    "x = x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Iteration 0\n",
      "Current loss value: 1179999600.0\n",
      "Iteration Time:  11.190200567245483\n",
      "Start of Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss value: 443627300.0\n",
      "Iteration Time:  8.03190565109253\n",
      "Start of Iteration 2\n",
      "Current loss value: 266511000.0\n",
      "Iteration Time:  8.210888147354126\n",
      "Start of Iteration 3\n",
      "Current loss value: 188589890.0\n",
      "Iteration Time:  8.100772142410278\n",
      "Start of Iteration 4\n",
      "Current loss value: 151635310.0\n",
      "Iteration Time:  8.202980756759644\n",
      "Start of Iteration 5\n",
      "Current loss value: 127149490.0\n",
      "Iteration Time:  8.437873601913452\n",
      "Start of Iteration 6\n",
      "Current loss value: 109568660.0\n",
      "Iteration Time:  8.099299907684326\n",
      "Start of Iteration 7\n",
      "Current loss value: 97293176.0\n",
      "Iteration Time:  8.129997253417969\n",
      "Start of Iteration 8\n",
      "Current loss value: 88165690.0\n",
      "Iteration Time:  8.063009023666382\n",
      "Start of Iteration 9\n",
      "Current loss value: 80935944.0\n",
      "Iteration Time:  8.16801118850708\n",
      "Start of Iteration 10\n",
      "Current loss value: 74897920.0\n",
      "Iteration Time:  8.126678466796875\n",
      "Start of Iteration 11\n",
      "Current loss value: 70108650.0\n",
      "Iteration Time:  8.085767269134521\n",
      "Start of Iteration 12\n",
      "Current loss value: 65960076.0\n",
      "Iteration Time:  8.134955406188965\n",
      "Start of Iteration 13\n",
      "Current loss value: 62606050.0\n",
      "Iteration Time:  8.126138687133789\n",
      "Start of Iteration 14\n",
      "Current loss value: 59659476.0\n",
      "Iteration Time:  8.12404489517212\n",
      "Start of Iteration 15\n",
      "Current loss value: 57551380.0\n",
      "Iteration Time:  8.08618426322937\n",
      "Start of Iteration 16\n",
      "Current loss value: 54733350.0\n",
      "Iteration Time:  8.190899133682251\n",
      "Start of Iteration 17\n",
      "Current loss value: 52309356.0\n",
      "Iteration Time:  8.202959299087524\n",
      "Start of Iteration 18\n",
      "Current loss value: 50947170.0\n",
      "Iteration Time:  8.09494662284851\n",
      "Start of Iteration 19\n",
      "Current loss value: 49592816.0\n",
      "Iteration Time:  8.154166460037231\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    print('Start of Iteration', i)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ### Runs the L-BFGS optimization over pixels of generated image to minimize the neural style loss.\n",
    "    ### 20 steps of gradient ascent per iteration \n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x, \n",
    "                                     fprime = evaluator.grads, maxfun = 20)\n",
    "    print(\"Current loss value:\", min_val)\n",
    "    img = x.copy().reshape((img_height, img_width, 3))\n",
    "    img = deprocess_image(img)\n",
    "    fname = result_prefix + '_iterations_%d.png'%i\n",
    "    imsave(fname, img)\n",
    "    end_time = time.time()\n",
    "    print('Iteration Time: ',end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

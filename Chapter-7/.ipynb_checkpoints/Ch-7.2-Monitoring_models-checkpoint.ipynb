{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "***Keras Callbacks and Tensorboard***\n",
    "\n",
    "------\n",
    "\n",
    "So until now we used set the model parameters and trained the model. Once the mode.fit() was called we didn't have any control on the model training. Most of the cases we ended up overfitting and then had to retrain. This is very impractical. Just to avoid this Keras callbacks can be used. The Tensorboard is a visualization tool where we can log the data and visualize all the parameters of the model as the model is getting trained. This can again help understand when we go wrong and abort the training at the right time. Also visualizing the weights histograms can tell a lot about how the model parameters distributions are. \n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "***Callbacks***\n",
    "\n",
    "-----\n",
    "\n",
    "When should we stop training the model ? \n",
    "The simple answer is when we see that the validation loss is not decreasing anymore. This is the point when the model is trained and starts overfitting to the training data. THe training error is always going to decrease as the optimization function we have written is meant to do so. \n",
    "\n",
    "We can achieve this by using Keras callbacks. A ***callback*** is an object that is passed to the model.fit(). So, when the model is training it will call this function during training. It has access to all the data, state of the model, performance, and it can take action, i.e. stop training, save model, load different weights set, or alter the state of the model. \n",
    "\n",
    "Some ways in which the callback ca be used are - \n",
    "\n",
    "- **Model Checkpointing** - Saving the current weights at different checkpoints during training. \n",
    "- **Early stopping** - Interrupting training when the validation loss is no more improving. \n",
    "- **Dynamically adjusting model parameters** - Adjusting the weights of the optimizer. \n",
    "- **Logging training and validation metrics during training, or visualizing the representation learned by the model as they've updated** \n",
    "\n",
    "------\n",
    "\n",
    "***Keras Callbacks***\n",
    "\n",
    "-----\n",
    "\n",
    "The keras callbacks include a number of built in callbacks. example keras.callbacks.ModelCheckpoint, keras.callbacks.EarlyStopping, etc. \n",
    "\n",
    "Let's dive in and understand them a little better on how they can they be used. \n",
    "\n",
    "```python\n",
    "import keras\n",
    "\n",
    "callbacks_list  = [keras.callbacks.EarlyStopping(monitor = 'acc', patience = 1,),\n",
    "                  keras.callbacks.ModelCheckpoint(filePath = 'my_model.h5', monitor = 'val_loss', save_best_only = True,)]\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "\n",
    "model.fit(x, y, epochs = 100, batch_size = 32, \n",
    "         callbacks = callbacks_list, validation_data = (x_val, y_val))\n",
    "```\n",
    "--------\n",
    "\n",
    "***ReduceLROnPlateau Callback***\n",
    "\n",
    "-------\n",
    "\n",
    "```python\n",
    "callbacks_list  = [keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 10,)]\n",
    "```\n",
    "\n",
    "The function reduces the learning rate by a factor of 10 as soon as the modelvalidation loss has stopped improving. \n",
    "\n",
    "------\n",
    "***Writing your own callback***\n",
    "\n",
    "-------\n",
    "If a specific action needs to be taken which is not covered in Keras callbacks, this function can be used. \n",
    "Callbacks are implemented by sub-classing the class *keras.callbacks.Callback*. \n",
    "\n",
    "It can be implemented in a number of ways, by calling at various points during the training like \n",
    "```python\n",
    "on_epoch_begin\n",
    "on_epoch_end\n",
    "\n",
    "on_batch_begin\n",
    "on_batch_end\n",
    "\n",
    "on_train_begin\n",
    "on_train_end\n",
    "```\n",
    "\n",
    "These methods are called using the logs argument, which is a dictionary containing information about previous batch, or epoch, or training run: training and validation metrics, and so on. \n",
    "\n",
    "The callbacks also has access to following attributes. \n",
    "\n",
    "```python\n",
    "self.model ## Instance from which the model is called\n",
    "self.validation_data ## The value of what was passed to fit as validation data\n",
    "```\n",
    "\n",
    "**Here is an example of custom callback that saves to disk the activations of every layer of model at the end of every epoch, calculated on first sample of validation data**\n",
    "\n",
    "```python\n",
    "import keras\n",
    "import numpy as np \n",
    "\n",
    "class ActivationLogger(keras.callbacks.Callback):\n",
    "    \n",
    "    ### Called by parent model before training, to inform the callback of what model will be calling it.\n",
    "    def set_model(self, model):\n",
    "        self.model - model\n",
    "        layer_outputs = [layer.output for layer in model.layers]\n",
    "        self.activations_model = keras.modelsModel(model.input, layer_outputs)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        if self.validation_data is None:\n",
    "            raise RuntimeError('Requires validation data')\n",
    "        validation_sample = self.validation_data[0][0:1]\n",
    "        activations = self.activations_model.predict(validation_sample)\n",
    "        f = open('activations_' + str(epoch) + '.npz', 'w')\n",
    "        np.savez(f, activations)\n",
    "        f.close()\n",
    "  ```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "***Tensorboard***\n",
    "\n",
    "----------------\n",
    "\n",
    "Tensorboard is a browser based visualization tool, where eseentially everything about the model can be visualized, the model graph, weights distribution, learning_rate graph, learning graphs, visualizing activations and gradients etc\n",
    "\n",
    "Let's start by using tensorboard. \n",
    "\n",
    "The tensorboard is hosted on localhost at port 6006. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "max_len = 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yash/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1008: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length = max_len, name = 'embed'))\n",
    "model.add(layers.Conv1D(32,7,activation = 'relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32,7,activation = 'relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937.0\n",
      "Trainable params: 291,937.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yash/anaconda3/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create a directory in the same folder where the tensorboard logs will be saved***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.TensorBoard(log_dir = 'my_log_dir')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s - loss: 0.6948 - acc: 0.5935 - val_loss: 0.6780 - val_acc: 0.7030\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.5310 - acc: 0.8036 - val_loss: 0.4340 - val_acc: 0.8300\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.4869 - acc: 0.8100 - val_loss: 0.4898 - val_acc: 0.8132\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.4446 - acc: 0.7963 - val_loss: 1.3151 - val_acc: 0.5738\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.4506 - acc: 0.7829 - val_loss: 0.5196 - val_acc: 0.7752\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.4238 - acc: 0.7676 - val_loss: 0.5990 - val_acc: 0.7256\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.4022 - acc: 0.7600 - val_loss: 0.5574 - val_acc: 0.7300\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.3814 - acc: 0.7220 - val_loss: 0.6075 - val_acc: 0.6894\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.3649 - acc: 0.6546 - val_loss: 1.1242 - val_acc: 0.4920\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.3394 - acc: 0.6430 - val_loss: 0.6869 - val_acc: 0.6062\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.3300 - acc: 0.6048 - val_loss: 0.6894 - val_acc: 0.5944\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.3276 - acc: 0.5564 - val_loss: 1.2645 - val_acc: 0.4302\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.2810 - acc: 0.5061 - val_loss: 0.9120 - val_acc: 0.4274\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.2682 - acc: 0.4242 - val_loss: 1.0279 - val_acc: 0.3822\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.2455 - acc: 0.3856 - val_loss: 1.1654 - val_acc: 0.3380\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.2055 - acc: 0.3424 - val_loss: 1.3416 - val_acc: 0.2784\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.1984 - acc: 0.2815 - val_loss: 1.3633 - val_acc: 0.2360\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.1833 - acc: 0.2213 - val_loss: 1.4302 - val_acc: 0.2152\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.1697 - acc: 0.1815 - val_loss: 1.6958 - val_acc: 0.1604\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 2s - loss: 0.1746 - acc: 0.1487 - val_loss: 1.6189 - val_acc: 0.1642\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 20, batch_size = 128, validation_split = 0.2, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The Tensorboard can be passed parameters like histograms, embedding_freq etc, for more analysis.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Keras also allows to save model graphs as images***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes = True, to_file = 'model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

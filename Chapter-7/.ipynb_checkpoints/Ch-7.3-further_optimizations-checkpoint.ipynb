{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "### Getting the most out of the models\n",
    "\n",
    "-----\n",
    "\n",
    "In this section the focus will be improve the model even further using advanced techniques, hyperparameter optimization, and MOdel ensembling\n",
    "\n",
    "------\n",
    "\n",
    "#### Advanced Architecture Patterns\n",
    "\n",
    "------\n",
    "\n",
    "Two more important concepts will be discussed in brief i.e. Normalization and depthwise seperable convolution. \n",
    "\n",
    "--------\n",
    "***Batch Normalization***\n",
    "\n",
    "---------\n",
    "*Normalization* is a broad category of methods that seek to make different samples seen by a ML Model more similar to each other, which helps the model learn and generalize better to new Data. \n",
    "\n",
    "We have considered Normalization before feeding the data to the Neural Networks by cenetering the data around 0 and keeping the range of values between (-1,1) using mean and std of the data. \n",
    "\n",
    "Once the data is passed to the model, the data after each transformation won't have the same distribution. \n",
    "\n",
    "So **BatchNormalization** layer was introduced in 2015, it can adaptively normallize the data even if the mean and variance of the data change over time over training. It works esentially by maintaining an exponential average of the batch_wise mean and variance of the data during training. The main advantage of Batch Normalization is that it helps gradient propogation. \n",
    "\n",
    "BatchNormalization layers are typically used after a Convolution layer or a Dense layer. \n",
    "\n",
    "The argument to the layer is axis which is the feature axis on which the Normalization needs to be performed. The default argument is (-1)\n",
    "\n",
    "\n",
    "***Batch Renormalization and selu*** - In 2017 the batch renormaliation paper was published which offers clear advantages over Batch Normalization. Also a new paper was introduced which uses 'selu' activation function and a specific initializer 'lecun_normal' which helps the dense layers to have Normalized activations without the need for external Normalization layers. The paper's name is 'Self normalizing Neural Networks'.\n",
    "\n",
    "-----------\n",
    "***Depth-Wise Seperable Convolution***\n",
    "\n",
    "-----\n",
    "This is a replacement of the vanilla Conv2D. This layer works on spatial convolutions on different channels seperately, before mixing the output channels using pointwise convolution. This is much more lighter and performs slightly better than the vanilla Conv2D. These advantages become especially important when we're training small models from scratch on limited data. \n",
    "\n",
    "We can replace all the Conv2D's with the following. \n",
    "\n",
    "```python\n",
    "layers.SeparableConv2D(64, 3, activation = 'relu)\n",
    "```\n",
    "\n",
    "The Xception model discussed in the last section also is based on the same concept.  \n",
    "\n",
    "---------\n",
    "***Hyperparameter Optimization***\n",
    "\n",
    "------\n",
    "\n",
    "The process of optimizing hyperparameters looks like:\n",
    "- Choose a set of hyperparameters based on intuition. \n",
    "- Build the corresponding model. \n",
    "- Train the model and check the model metrics. \n",
    "- Tune the model parameters based on model metrics and intuition. \n",
    "- Keep repeating. Eventually measure on test data. \n",
    "\n",
    "\n",
    "***The hyperparameter space is typically made of discrete decisions and thus isn't continous or differentiable. Hence, you typically can't do gradient descent in hyperparameter space. So we have to resort to gradient free optimization techniques.***\n",
    "\n",
    "Hyperparameter tuning should be done in order to make sure the model generalizes better. In this process we have to make sure that we are not trying to overfit the model with respect to the validation set. \n",
    "\n",
    "--------\n",
    "***Model Ensembling***\n",
    "\n",
    "-------\n",
    "\n",
    "Ensembling is pooling together predictions from different models on the same problem. Here our goal is to capture the best of all models. So this esentially relies on the fact that different models trained are good for different reasons. \n",
    "\n",
    "There are a lot of ensembling techniques used. Some of them are - \n",
    "1. Finding average of all model's predictions. \n",
    "2. Finding weighted average of all model's predictions. \n",
    "3. Maximum voting\n",
    "4. Bagging\n",
    "\n",
    "THe concept of Ensembling completely depends on the nature of the data that we have. So based on the different models preformance over the data goal is to have a stronger model. Ensembling is not how good my best model is, rather it is about the diversity of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Branches of ML - \n",
    "\n",
    "1. Supervised Learning - In this case we have annotated data, i.e. targets available directly against which we train the model. Examples are Object Detection, Sequence Generation, Image Segmentation, etc\n",
    "2. Unsupervised Learning - It is where pattern in data is looked for as no annotations are availbale. The most common algorithms are Dimensionality reduction, Clustering etc.\n",
    "3. Semi-supervised Learning - It is a supervised learning technique where no human annotated data is available. Here the annotations are not directly available. For example consider autoencoders where the target image is same as the input image. \n",
    "3. Reinforcement Learning - An agent recieves a reward based on perfoming some action in the environment. This is training the agent to maximize the reward function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to keep in mind\n",
    "\n",
    "1. Data Representativeness - The way in which the data is being split between the two i.e. the training set, and the testing set needs to be done carefully. One thing to look for is there should be no overlap between them. Their distribution must be same. So one usual way to make sure of the above two things is that the data is randomly shuffled and then split. \n",
    "2. This technique cannot be applied to temporal data as this will cause temporal leak. So here the data can be split by keeping the test data in the posterior to the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "1. **Vectorization** - All input data must be floating point data tensors(or in some specific cases tensor of integers). All data needs to be converted to tensors. \n",
    "2. **Value Normalization** - The values being fed to the network must be homogenous and have small values. Because having large values may cause the network to calculate large gradient values and prevent the model from fitting. So feature normalization is usually done i.e. all features usually within a small range, example 0-1, and all features should have roughly the same range. \n",
    "3. Make sure there is no missing data. If there are some rows where you have null values. Use the best judgement according to the dataset to manage the null values. In a very large dataset removing some vectors may not harm training. Sometimes the nature of the data is also used to fill in these values. Also as a baseline method the values are usually filled using median or mean values. \n",
    "4. **Feature Engineering** - There are many problems which can be solved by just smart feature engineering. Consider the problem of telling the time by looking at the clock. This problem can be done by passing the image data to the CNN, But that would be using a lot of resources. Instead a simple model can just using the angle made by the two hands and find the time without using any deep learning techniqes. So it is important to feature analyze the data in such a way that model does not take up more resources, and the new features engineered better represent the problem or clear the noise or irrelevent information from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting\n",
    "\n",
    "Overfitting is the problem where the model starts to lose its generalizing capabilities and starts mapping the training space exactly. So to avoid overfitting some measure can be adopted. \n",
    "1. Use validation set which is totally independent of training set to validate the model performance periodically. Use early stopping based on validation loss. Make sure there are no leaks from the validation set into the model training. \n",
    "2. Designing the appropriate size model. Using a very large model for simple problem might cause overfitting as the model quickly maps itself to the training data exactly. We need the model to be general. So picking the right model size is also necessary. Using very small model might cause underfitting of the model. So, to balance the problem out ttrying different models with varying parameters can help understand the problem better. \n",
    "3. Using regularization and dropouts are another way to make sure that the model remains general. \n",
    "    - L1 Regularization - THe cost added is directly proportonal to the model parameters. \n",
    "    - L2 Regularization - The cost added is proportional to the sum of squares of the model parameters. THis can also be considered as weight decay. \n",
    "    \n",
    "**In Keras the regularization term can be added in the following way**\n",
    "> from keras import regularizers\n",
    "> layers.Dense(16, **kernal_regularizer=regularizers.l2(0.001)**, activation='relu')\n",
    "\n",
    "Both regularizers can also be applied simultanously. \n",
    "> regularizers.l1_l2(l1=0.001,l2=0.001)\n",
    "\n",
    "**In Keras Dropout is added in following way**\n",
    "\n",
    "> from keras import dropout\n",
    "> layers.Dropout(0.5)\n",
    "\n",
    "Dropout randomly drops that percentage of the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach to Problem Solving \n",
    "\n",
    "1. **Define the problem and assemble the dataset** - Understand the nature of the problem, what the input data is, what the output is, the distribution of data. THis would help in feature engineering and also understanding what kind of problem are we facing, i.e. binary classification, regression, etc.\n",
    "    - The nature of the data can also be used as the feature when training the model. For example when deading with seasonal data the season can be passed as a sepearte feature which can be calculated using date column. These all things are a part of feature engineering. \n",
    "\n",
    "2. **Choosing a measure of success** - The measure most commonly used to judge a model is accuracy. But when dealing with problems in NLP there are a lot of different measures which come up. For example Precision and Recall curves are important part. Consider the problem of Face Recognition for Security purposes, here the false negatives are not as relevant as false positives. You may not want unauthorized person to get an access to something. So the measure of the success is defined by the problem statement itself. Another common measure frequently used is Area Under the Reciever Operating Characterstics Curve. This is usually used for binary classification problems to analyze the model success. \n",
    "\n",
    "3. **Deciding Evaluation Protocol** - What validation tecniques to use. How to use them and when to use them are the important questions which need to be answered before designing the model. So for example when plenty of data is available the hold out validation split can be taken whereas when the data available is very less then we might need to opt for Iterated K-fold validation.\n",
    "\n",
    "4. **Prepare Data** - Once above questions have been analyzed the data processing can be done after feature engineering. The data needs to be converted to tensors, and brought within a small range. ALso homogeniety of the data needs to be maintained. \n",
    "\n",
    "5. **Start Small** - Develop a model that performs better than the baseline statistical measure. FOr instane getting a accuracy of greater than 10 percent on the MNIST dataset is basically beating the random classifier. \n",
    "\n",
    "6. **Important Choices** - Three important choices for every model are - \n",
    "        - Last Layer Activation - Depends upon the nature of the problem.\n",
    "        - Loss FUnction - Again depends upon the nature of the problem.\n",
    "        - Optimization Configuration - Deciding optimizer and learning rate. \n",
    "7. **Scale Up** - Once you have a base model ready. Try adding more layers, changing hyperparameters, making layers bigger, and observe model characterstics like training and validation metrics and make changes accordingly. Use different regularization techniques in moderation. Too much regularization may damage model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If you're test accuracy is way worse than the validation accuracy then one of the most common reasons might be improper data processing, or improper validation procedure. Your model might be overfitting to the validation set due to validation leaks.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
